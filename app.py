import streamlit as st
import re
import html
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from email.utils import parsedate_to_datetime
import pandas as pd

# =========================
# 0) API ì •ë³´ (ë³´ì•ˆìƒ í™˜ê²½ë³€ìˆ˜ ê¶Œì¥)
# =========================
CLIENT_ID = "3bewxYUlBRRcl9j2X4AK"
CLIENT_SECRET = "dDgX44GYmS"
NAVER_NEWS_API = "https://openapi.naver.com/v1/search/news.json"

# =========================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
# =========================
def strip_html_tags(text):
    text = html.unescape(text or "")
    return re.sub(r"<[^>]+>", "", text).strip()

def normalize_spaces(s):
    s = (s or "").replace("\xa0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def contains_any_keyword(text, keywords):
    t = (text or "").lower()
    return any(k.strip().lower() in t for k in keywords if k.strip())

def format_pubdate(pubdate_str):
    try:
        dt = parsedate_to_datetime(pubdate_str)
        return dt.strftime("%Y-%m-%d")
    except: return ""

# (ìƒëµëœ í¬ë¡¤ë§ í•¨ìˆ˜ë“¤: fetch_press_and_body, extract_naver_body ë“±ì€ ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ìœ ì§€)
# ì—¬ê¸°ì— ê¸°ì¡´ ì½”ë“œì˜ 3)ë²ˆê³¼ 4)ë²ˆ í•¨ìˆ˜ë“¤ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ì„œ ë„£ì–´ì£¼ì„¸ìš”.

def fetch_press_and_body(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers, timeout=10)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        
        # ê°„ë‹¨í•œ ì¶”ì¶œ ë¡œì§ (ê¸°ì¡´ í•¨ìˆ˜ í™œìš©)
        press = "ì•Œ ìˆ˜ ì—†ìŒ"
        meta_press = soup.select_one("meta[property='og:site_name']")
        if meta_press: press = meta_press['content']
        
        body = soup.get_text(" ", strip=True)
        return press, body
    except:
        return "ì ‘ê·¼ë¶ˆê°€", ""

# =========================
# 5) Streamlit UI êµ¬í˜„
# =========================
st.set_page_config(page_title="ë‰´ìŠ¤ í‚¤ì›Œë“œ ìˆ˜ì§‘ê¸°", layout="wide")

st.title("ğŸ“° ì‹¤ì‹œê°„ ë‰´ìŠ¤ í‚¤ì›Œë“œ ìˆ˜ì§‘ê¸°")
st.markdown("ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¥¼ í™œìš©í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ë³¸ë¬¸ì„ í•„í„°ë§í•©ë‹ˆë‹¤.")

with st.sidebar:
    st.header("ì„¤ì •")
    # ì‚¬ìš©ìë¡œë¶€í„° í‚¤ì›Œë“œ ì…ë ¥ ë°›ê¸° (ì‰¼í‘œë¡œ êµ¬ë¶„)
    user_keywords = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„)", "ë°€ë¼ë…¸, ì˜¬ë¦¼í”½, ê¸‰ì‹")
    display_count = st.slider("ê²€ìƒ‰ ê±´ìˆ˜", 10, 100, 50)
    start_button = st.button("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")

if start_button:
    keywords = [k.strip() for k in user_keywords.split(",")]
    query = " ".join(keywords)
    
    headers = {"X-Naver-Client-Id": CLIENT_ID, "X-Naver-Client-Secret": CLIENT_SECRET}
    params = {"query": query, "display": display_count, "sort": "date"}
    
    with st.spinner('ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...'):
        response = requests.get(NAVER_NEWS_API, headers=headers, params=params)
        
        if response.status_code == 200:
            items = response.json().get("items", [])
            results = []
            
            progress_bar = st.progress(0)
            for idx, it in enumerate(items):
                title = strip_html_tags(it.get("title", ""))
                link = it.get("link", "")
                pub_date = format_pubdate(it.get("pubDate", ""))
                
                # ë³¸ë¬¸ ìˆ˜ì§‘
                press, body = fetch_press_and_body(link)
                
                # í•„í„°ë§
                if contains_any_keyword(title + " " + body, keywords):
                    results.append({
                        "ë‚ ì§œ": pub_date,
                        "ì–¸ë¡ ì‚¬": press,
                        "ì œëª©": title,
                        "URL": link
                    })
                
                # ì§„í–‰ë¥  í‘œì‹œ
                progress_bar.progress((idx + 1) / len(items))
                time.sleep(0.1)
            
            # ê²°ê³¼ ì¶œë ¥
            st.success(f"ì´ {len(results)}ê±´ì˜ ë‰´ìŠ¤ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            df = pd.DataFrame(results)
            
            # í…Œì´ë¸” ì¶œë ¥ ë° ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
            st.dataframe(df, use_container_width=True)
            
            csv = df.to_csv(index=False).encode('utf-8-sig')
            st.download_button("CSV ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", csv, "news_result.csv", "text/csv")
        else:
            st.error("API ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. IDì™€ Secretì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
